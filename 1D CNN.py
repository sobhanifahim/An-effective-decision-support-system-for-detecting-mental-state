# -*- coding: utf-8 -*-
"""Synthetic_data_EEG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Kb-SnhTp9Hg-BDYsqg-sNgRRLVMOoLoz
"""

import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import LinearSVC
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn import metrics
import keras
from keras.layers import Conv1D
from keras.layers import Dense, Activation, Flatten, Conv1D, Dropout, BatchNormalization, MaxPooling1D, LeakyReLU
from numpy import mean
from numpy import std
from numpy import dstack
from matplotlib import pyplot
from keras.models import Sequential
from matplotlib.pyplot import figure
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import Dropout
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.utils import to_categorical
from keras.optimizers import Adam
from keras.layers import Dense,Embedding,SimpleRNN,Dense,Dropout
from keras import metrics, regularizers
from keras.utils.vis_utils import plot_model

df=pd.read_csv("/content/drive/MyDrive/400C_dataset/synthetic_df.csv")
df

l=LabelEncoder()

df['Gender']=l.fit_transform(df['Gender'])

df.head()

df['Mind State'].value_counts()

df['Mind State'].hist(bins=50,figsize=(25,5),alpha=0.6)
plt.show()

for i in range(df.shape[1]):
    n_miss = df.iloc[:, i].isnull().sum()
    perc = n_miss / df.shape[0] * 100
    print('> %s, Missing: %d (%.3f%%)' % (df.columns[i], n_miss, perc))

df["high_Alpha(Hz)"].fillna( method ='ffill', limit = 1, inplace = True)
df["high_Beta(Hz)"].fillna( method ='ffill', limit = 1, inplace = True)
df["low_Gamma(Hz)"].fillna( method ='ffill', limit = 1, inplace = True)

for i in range(df.shape[1]):
    n_miss = df.iloc[:, i].isnull().sum()
    perc = n_miss / df.shape[0] * 100
    print('> %s, Missing: %d (%.3f%%)' % (df.columns[i], n_miss, perc))

df = df.drop(['Patient_ID'], axis=1)

df.describe()

df.groupby("Mind State").describe()

import sys
np.set_printoptions(threshold=sys.maxsize)

e=df['Mind State']
le = LabelEncoder()
e = le.fit_transform(e)

e

X1 = df.drop('Mind State', axis=1)
Y1=np.array(e)

X_train, X_test, y_train, y_test = train_test_split(X1, Y1, test_size=0.30, random_state=77)
print(len(X_train))
print(len(X_test))
print(len(y_train))
print(len(y_test))

X_train=X_train.astype(np.float32)
y_train=y_train.astype(np.float32)
X1_train=X_train
y1_train=y_train

X_train.shape

ms_input_shape = (1,8) 
X_train = np.random.random((2,1,8))
y_train = np.random.random((2,8))
model = keras.models.Sequential()
model.add(Conv1D(filters=18, kernel_size=3, strides=1, padding='same', activation='relu', input_shape= ms_input_shape,kernel_initializer=keras.initializers.he_normal()))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))
model.add(Conv1D(filters=32, kernel_size=5, strides=1, padding='same',activation='relu'))
model.add(BatchNormalization())
model.add(MaxPooling1D(pool_size=2, strides=2, padding='same'))
model.add(Flatten())
model.add(Dense(120, activation='relu'))
model.add(Dense(100))
model.add(Dense(8, activation='softmax'))
model.summary()

plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

import tensorflow as tf
model.compile(optimizer=tf.keras.optimizers.Adam(),loss='categorical_crossentropy',metrics=['acc'])


callbacks_list = [
    keras.callbacks.ModelCheckpoint(
        filepath='best_model.{epoch:02d}-{val_loss:.2f}.h5',
        monitor='val_loss', save_best_only=True),
    keras.callbacks.EarlyStopping(monitor='acc', patience=1)]

BATCH_SIZE = 32 
EPOCHS = 100

history = model.fit(X_train,
                    y_train,
                    batch_size=BATCH_SIZE,
                    validation_split=0.2,
                    epochs=EPOCHS,
                    verbose=1)

