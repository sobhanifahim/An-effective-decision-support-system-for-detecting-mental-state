# -*- coding: utf-8 -*-
"""EEG2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cPdJPk9YyKO1ifvIK20Xwo1OYiMoNePb
"""

from lightgbm import LGBMClassifier
from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier,StackingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score
from sklearn.model_selection import GridSearchCV,KFold,train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from time import time
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense,Dropout, LSTM
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import validation_curve
from sklearn.model_selection import learning_curve
from sklearn.datasets import load_digits

df=pd.read_csv('/content/drive/MyDrive/400C_dataset/synthetic_df.csv')
df

df=df.drop("Patient_ID",axis=1)

for i in range(df.shape[1]):
    n_miss = df.iloc[:, i].isnull().sum()
    perc = n_miss / df.shape[0] * 100
    print('> %s, Missing: %d (%.3f%%)' % (df.columns[i], n_miss, perc))

df["high_Alpha(Hz)"].fillna( method ='ffill', limit = 1, inplace = True)
df["high_Beta(Hz)"].fillna( method ='ffill', limit = 1, inplace = True)
df["low_Gamma(Hz)"].fillna( method ='ffill', limit = 1, inplace = True)

le = LabelEncoder()
df['Mind State']= le.fit_transform(df['Mind State'])
df['Gender']=le.fit_transform(df['Gender'])

df

X1 = df.drop('Mind State', axis=1)
Y1=df['Mind State']

kf = KFold(n_splits=5)
ratio=0.70

for train_index,test_index in kf.split(X1):
    X_train,X_test = X1.iloc[train_index],X1.iloc[test_index]
    y_train,y_test = Y1.iloc[train_index],Y1.iloc[test_index]
    X_val,X_test,y_val,y_test = train_test_split(X_test, y_test, test_size=0.75, random_state=42)

scaler = StandardScaler()
scaler.fit(X_train)

features = X_train.columns

X_train[features] = scaler.transform(X_train)
X_val[features] = scaler.transform(X_val)
X_test[features] = scaler.transform(X_test)

print(len(X_train[features]))

print(len(X_test[features]))

print(len(X_val[features]))

plt.figure(figsize=(14,8))
matrix = np.triu(X_train[features].corr())
sns.heatmap(X_train[features].corr(), annot=True, fmt=".2f", vmin=-1, vmax=1, center=0, cmap="coolwarm", mask=matrix);

sns.pairplot(df,hue='Mind State',data=df);
plt.rcParams.update({'font.size': 30})
plt.savefig("pplot.png")

"""Logistic regression"""

parameters = {
    "C":[0.001,0.01,0.1,1.0,10,100,1000],
    "penalty":["l1","l2","elasticnet",None],
    "solver":["newton-cg","lbfgs","liblinear","sag","saga"]
}

lr_cv = GridSearchCV(LogisticRegression(), parameters, cv=5)
lr_cv.fit(X_train[features],y_train.values.ravel())

lr = lr_cv.best_estimator_
lr.fit(X_train[features],y_train.values.ravel())

"""Decision Tree"""

parameters = {
    "criterion":["gini","entropy"],
    "max_depth":[2,4,8,16,32],
    "min_samples_leaf":[2,4,8,16,32],
    "min_samples_split":[2,4,8,16,32]
}

dt_cv = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5)
dt_cv.fit(X_train[features],y_train.values.ravel())

dt = dt_cv.best_estimator_
dt.fit(X_train[features],y_train.values.ravel())

"""Random Forest"""

parameters = {
    "max_depth":[2,4,8,16,32],
    "n_estimators":[5,50,250,500]
}

rf_cv = GridSearchCV(RandomForestClassifier(), parameters, cv=5)
rf_cv.fit(X_train[features],y_train.values.ravel())

rf = rf_cv.best_estimator_
rf.fit(X_train[features],y_train.values.ravel())

"""Ada Boost"""

parameters = {
    "learning_rate":[0.01,0.1,1.0,10,100],
    "n_estimators":[5,50,250,500]
}

ada_cv = GridSearchCV(AdaBoostClassifier(), parameters, cv=5)
ada_cv.fit(X_train[features],y_train.values.ravel())

ada = ada_cv.best_estimator_
ada.fit(X_train[features],y_train.values.ravel())

"""Light Gradient Boosting Machine"""

parameters = {  
    "boosting_type":["gbdt","dart","goss","rf"],
    "learning_rate":[0.01,0.1,1.0,10,100],
    "max_depth":[2,4,8,16,32],
    "n_estimators":[5,50,250,500]
}

lgbm_cv = GridSearchCV(LGBMClassifier(), parameters, cv=5)
lgbm_cv.fit(X_train[features],y_train.values.ravel())

lgbm = lgbm_cv.best_estimator_
lgbm.fit(X_train[features],y_train.values.ravel())

"""KNN"""

parameters = {
    "algorithm":["ball_tree","kd_tree","brute"],
    "metric":["minkowski","euclidean","manhattan"],
    "n_neighbors":range(2,21),
    "weights":["uniform","distance"]
}

knn_cv = GridSearchCV(KNeighborsClassifier(), parameters, cv=5)
knn_cv.fit(X_train[features],y_train.values.ravel())

knn = knn_cv.best_estimator_
knn.fit(X_train[features],y_train.values.ravel())

"""Support Vector Machine"""

parameters = {
    "C":[0.001,0.01,0.1,1.0,10,100,1000],
    "kernel":["linear","poly","rbf","sigmoid"]
}

svc_cv = GridSearchCV(SVC(), parameters, cv=5)
svc_cv.fit(X_train[features],y_train.values.ravel())

svc = svc_cv.best_estimator_
svc.fit(X_train[features],y_train.values.ravel())

"""Multi-layer-Perceptron(Deep ANN)"""

parameters = {
    "activation":["identity","logistic","tanh","relu"],
    "hidden_layer_sizes":[(10,),(50,),(100,)],
    "learning_rate":["constant","invscaling","adaptive"],
    "solver":["lbfgs","sgd","adam"]
}

mlp_cv = GridSearchCV(MLPClassifier(), parameters, cv=5)
mlp_cv.fit(X_train[features],y_train.values.ravel())

mlp = mlp_cv.best_estimator_
mlp.fit(X_train[features],y_train.values.ravel())

estimators = [("lr",lr),("dt",dt),("rf",rf),("ada",ada),("lgbm",lgbm),("knn",knn),("svc",svc),("mlp",mlp)]
parameters = {
    "passthrough":[True,False]
}

sc_cv = GridSearchCV(StackingClassifier(estimators=estimators, final_estimator=lr_cv.best_estimator_), parameters, cv=5)
sc_cv.fit(X_train[features],y_train.values.ravel())

sc = sc_cv.best_estimator_
sc.fit(X_train[features],y_train.values.ravel())

models = [lr,dt,rf,ada,lgbm,knn,svc,mlp,sc]
val_set = pd.DataFrame()
    
for i in models:
        start = time()
        pred = i.predict(X_val[features])
        end = time()        
        temp = pd.DataFrame(
                {
                    "Accuracy":("%0.3f" % (accuracy_score(y_val,pred))),
                    "F1":("%0.3f" % (f1_score(y_val,pred,average='micro'))),
                    "Precision":("%0.3f" % (precision_score(y_val,pred,average='micro'))),
                    "Recall":("%0.3f" % (recall_score(y_val,pred,average='micro'))),
                    "Latency":("%0.1fms" % ((end-start)*1000))
                }, index=[str(i).split("Classifier")[0].split("(")[0]]
        )
        val_set = pd.concat([val_set,temp])
val_set

test_set = pd.DataFrame()
    
for i in models:
        start = time()
        pred = i.predict(X_test[features])
        end = time()        
        temp = pd.DataFrame(
                {
                    "Accuracy":("%0.3f" % (accuracy_score(y_test,pred))),
                    "F1":("%0.3f" % (f1_score(y_test,pred,average='micro'))),
                    "Precision":("%0.3f" % (precision_score(y_test,pred,average='micro'))),
                    "Recall":("%0.3f" % (recall_score(y_test,pred,average='micro'))),
                    "Latency":("%0.1fms" % ((end-start)*1000))
                }, index=[str(i).split("Classifier")[0].split("(")[0]]
        )
        test_set = pd.concat([test_set,temp])
test_set

train_set = pd.DataFrame()
    
for i in models:
        start = time()
        pred = i.predict(X_train[features])
        end = time()        
        temp = pd.DataFrame(
                {
                    "Accuracy":("%0.3f" % (accuracy_score(y_train,pred))),
                    "F1":("%0.3f" % (f1_score(y_train,pred,average='micro'))),
                    "Precision":("%0.3f" % (precision_score(y_train,pred,average='micro'))),
                    "Recall":("%0.3f" % (recall_score(y_train,pred,average='micro'))),
                    "Latency":("%0.1fms" % ((end-start)*1000))
                }, index=[str(i).split("Classifier")[0].split("(")[0]]
        )
        train_set = pd.concat([train_set,temp])
train_set


